{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid / Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The sigmoid function squishes all input values between 0 and 1. It is used to convert the output of a linear function\n",
    "into a probability, which is then used to make a binary decision. \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "z : numpy array\n",
    "\n",
    "Returns\n",
    "-------\n",
    "_ : numpy array\n",
    "    the predicted output\n",
    "'''\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    return 1.0/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The loss function is the overall loss function for logistic regression for this specific iteration. \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y : numpy array\n",
    "y_hat : numpy array\n",
    "\n",
    "Returns\n",
    "-------\n",
    "_ : float\n",
    "    the loss value of that particular iteration\n",
    "'''\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    \n",
    "    return -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The gradients function calculates the partial derivatives of the loss function with respect to the weights and bias.\n",
    "This partial derivative is used to update the weights and bias in the direction that minimizes the loss function.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X, inputs : numpy array\n",
    "y, actual values : numpy array\n",
    "y_hat, hypothesis/predictions : numpy array\n",
    "\n",
    "Returns\n",
    "-------\n",
    "dw : float\n",
    "    partial derivative of the loss function with respect to the weights\n",
    "db : float\n",
    "    partial derivative of the loss function with respect to the bias\n",
    "'''\n",
    "\n",
    "def gradients(X, y, y_hat):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dw = (1/m)*np.dot(X.T, (y_hat - y))\n",
    "    db = (1/m)*np.sum((y_hat - y)) \n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The normalise function normalises the input features by subtracting the mean and dividing by the standard deviation.\n",
    "This helps to scale down the input features to a common scale, which helps in faster convergence of the gradient \n",
    "descent algorithm, and reduces the magnitude of the weights.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X, inputs : numpy array\n",
    "\n",
    "Returns\n",
    "-------\n",
    "X : numpy array\n",
    "    NumPy array of normalised input features\n",
    "'''\n",
    "\n",
    "def normalize(X):\n",
    "\n",
    "    _, n = X.shape\n",
    "    \n",
    "    for i in range(n):\n",
    "        X = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The train function trains the logistic regression model using the input features and target values. It uses the sigmoid \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X, inputs : numpy array\n",
    "y, actual values : numpy array\n",
    "bs, batch size : int\n",
    "epochs, number of iterations : int\n",
    "lr, learning rate : float\n",
    "\n",
    "Returns\n",
    "-------\n",
    "w : numpy.ndarray\n",
    "    The learned weights of the logistic regression model (shape: (n, 1)).\n",
    "b : float\n",
    "    The learned bias term of the logistic regression model.\n",
    "losses : list of floats\n",
    "    A list containing the loss values for each epoch during training.\n",
    "'''\n",
    "\n",
    "def train(X, y, bs, epochs, lr):\n",
    "        \n",
    "    # m-> number of training examples\n",
    "    # n-> number of features \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias to zeros.\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    \n",
    "    # Reshaping y.\n",
    "    y = y.reshape(m,1)\n",
    "    \n",
    "    # Normalizing the inputs.\n",
    "    x = normalize(X)\n",
    "    \n",
    "    # Empty list to store losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop.\n",
    "    for _ in range(epochs):\n",
    "        for i in range((m-1)//bs + 1):\n",
    "            \n",
    "            # Defining batches. SGD.\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # Calculating hypothesis/prediction.\n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            # Getting the gradients of loss w.r.t parameters.\n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            \n",
    "            # Updating the parameters.\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "        \n",
    "        # Calculating loss and appending it in the list.\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(l)\n",
    "        \n",
    "    # returning weights, bias and losses(List).\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable w that is the weight. should be initialized to a zero vector of the same length as the number of features\n",
    "# create variable b that is the bias. should be initialized to 0\n",
    "\n",
    "# y_hat is the predicted value of y. It is calculated by taking the dot product of the weight vector, w and the feature vector X followed by the addition of the bias, b.\n",
    "y_hat = sigmoid(w.X + b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "50.007_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
