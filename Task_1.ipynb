{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) Own Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Sigmoid function used to convert the output of a linear function into a probability, which is then used to make a binary decision. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : Real number\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _: float\n",
    "        Output value between 0 and 1\n",
    "    '''\n",
    "    \n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    '''\n",
    "    The loss function computes the loss for logistic regression for a single training example. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy array\n",
    "    y_hat : numpy array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _ : float\n",
    "        the loss value of that particular training example\n",
    "    '''\n",
    "    \n",
    "    return np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "    '''\n",
    "    The gradients function calculates the partial derivatives of the loss function with respect to the weights and bias.\n",
    "    This partial derivative is used to update the weights and bias in the direction that minimizes the loss function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, inputs : numpy array\n",
    "    y, actual values : numpy array\n",
    "    y_hat, hypothesis/predictions : numpy array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dw : float\n",
    "        partial derivative of the loss function with respect to the weights\n",
    "    db : float\n",
    "        partial derivative of the loss function with respect to the bias\n",
    "    '''\n",
    "\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dw = (1 / m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1 / m) * np.sum((y_hat - y))\n",
    "\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    '''\n",
    "    The normalise function normalises the input features by subtracting the mean and dividing by the standard deviation.\n",
    "    This helps to scale down the input features to a common scale, which helps in faster convergence of the gradient \n",
    "    descent algorithm, and reduces the magnitude of the weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, inputs : numpy array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy array\n",
    "        NumPy array of normalised input features\n",
    "    '''\n",
    "\n",
    "    epsilon=1e-8\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    X_normalized = (X - mean) / (std + epsilon)\n",
    "    \n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, bs, epochs, lr):\n",
    "    '''\n",
    "    The train function trains the logistic regression model using the input features and target values. It uses the sigmoid function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, inputs : numpy array\n",
    "    y, actual values : numpy array\n",
    "    bs, batch size : int\n",
    "    epochs, number of iterations : int\n",
    "    lr, learning rate : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : numpy.ndarray\n",
    "        The learned weights of the logistic regression model (shape: (n, 1)).\n",
    "    b : float\n",
    "        The learned bias term of the logistic regression model.\n",
    "    losses : list of floats\n",
    "        A list containing the loss values for each epoch during training.\n",
    "    '''\n",
    "    # m: no. of training examples\n",
    "    # n: no. of features \n",
    "    m, n = X.shape\n",
    "    # weight\n",
    "    w = np.zeros((n, 1))\n",
    "    # bias\n",
    "    b = 0\n",
    "    y = y.reshape(m, 1)\n",
    "    X = normalize(X)\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for i in range((m + bs - 1) // bs):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            # xb: batch of input features for the specific batch\n",
    "            # yb: batch of target values for the specific batch\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(-l)\n",
    "\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    '''\n",
    "    The predict function uses the learned weights and bias to make predictions on the input features.\n",
    "    The inputs should be either the validation set or the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, inputs : numpy array\n",
    "    w, learned weights : numpy array\n",
    "    b, learned bias : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _ : numpy array\n",
    "        the predicted output contating 0s and 1s.\n",
    "    '''\n",
    "\n",
    "    global w, b\n",
    "    \n",
    "    X  = normalize(X)\n",
    "\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    pred_class = [1 if i >= 0.5 else 0 for i in preds]\n",
    "    \n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./data/train_tfidf_features.csv\")\n",
    "X = df_train.drop(['label', 'id'], axis=1)\n",
    "y = df_train['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b) Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\65911\\AppData\\Local\\Temp\\ipykernel_29204\\1587168080.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
      "C:\\Users\\65911\\AppData\\Local\\Temp\\ipykernel_29204\\1587168080.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  return np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s for own implementation:  1254\n",
      "Number of 0s for own implementation:  2183\n"
     ]
    }
   ],
   "source": [
    "w, b, losses = train(X_train.values, y_train.values, bs=32, epochs=100, lr=0.01)\n",
    "\n",
    "self_y_pred = predict(X_val.values)\n",
    "num_ones = np.count_nonzero(self_y_pred)\n",
    "num_zeros = len(self_y_pred) - num_ones\n",
    "print(\"Number of 1s for own implementation: \", num_ones)\n",
    "print(\"Number of 0s for own implementation: \", num_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SkLearn Version (for comparison only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s for SKLearn:  914\n",
      "Number of 0s for SKLearn:  2523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = Pipeline(steps=[('regressor', LogisticRegression())])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "sklearn_y_pred = model.predict(X_val)\n",
    "\n",
    "num_ones = np.count_nonzero(sklearn_y_pred)\n",
    "num_zeros = len(sklearn_y_pred) - num_ones\n",
    "print(\"Number of 1s for SKLearn: \", num_ones)\n",
    "print(\"Number of 0s for SKLearn: \", num_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.6718068082630201\n",
      "precision_score: 0.5781499202551834\n",
      "recall_score: 0.547583081570997\n",
      "f1_score: 0.5624515128006207\n",
      "confusion_matrix: [[1584  529]\n",
      " [ 599  725]]\n",
      "accuracy_score: 0.7131219086412569\n",
      "precision_score: 0.6849015317286652\n",
      "recall_score: 0.472809667673716\n",
      "f1_score: 0.5594280607685433\n",
      "confusion_matrix: [[1825  288]\n",
      " [ 698  626]]\n"
     ]
    }
   ],
   "source": [
    "for y_pred in [self_y_pred, sklearn_y_pred]:\n",
    "    for metric in [accuracy_score, precision_score, recall_score, f1_score, confusion_matrix]:\n",
    "        print(f\"{metric.__name__}: {metric(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: There is no significant difference in the mean prediction errors between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImplementedLogisticRegressionModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.loss_history = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "    def loss(self, y, y_hat):\n",
    "        return -np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) / len(y)\n",
    "    \n",
    "    def gradients(self, X, y, y_hat):\n",
    "        m = X.shape[0]\n",
    "        dw = (1 / m) * np.dot(X.T, (y_hat - y))\n",
    "        db = (1 / m) * np.sum(y_hat - y)\n",
    "        return dw, db\n",
    "    \n",
    "    def normalize(self, X):\n",
    "        epsilon = 1e-8\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "        X_normalized = (X - mean) / (std + epsilon)\n",
    "        return X_normalized\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.normalize(X)\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros((n, 1))\n",
    "        self.bias = 0\n",
    "        y = y.reshape(m, 1)\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_hat = self.sigmoid(z)\n",
    "            loss = self.loss(y, y_hat)\n",
    "            dw, db = self.gradients(X, y, y_hat)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.normalize(X)\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_hat = self.sigmoid(z)\n",
    "        y_pred = np.round(y_hat)\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"learning_rate\": self.learning_rate, \"num_iterations\": self.num_iterations}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented model scores: [0.58862325 0.58794385 0.58718573 0.60260586 0.61097461 0.60635697\n",
      " 0.60967742 0.60661157 0.59868421 0.58536585]\n",
      "Scikit-learn model scores: [0.55813953 0.58221024 0.57641921 0.62412587 0.59358289 0.58145815\n",
      " 0.5844504  0.59127337 0.57506824 0.55197133]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "scoring = make_scorer(f1_score)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "your_model_scores = cross_val_score(ImplementedLogisticRegressionModel(), X.values, y.values, cv=skf, scoring=scoring)\n",
    "sklearn_model_scores = cross_val_score(LogisticRegression(), X, y, cv=skf, scoring=scoring)\n",
    "\n",
    "print(f\"Implemented model scores: {your_model_scores}\")\n",
    "print(f\"Scikit-learn model scores: {sklearn_model_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 2.3418865540881373\n",
      "P-value: 0.030886855352741792\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t_stat, p_value = ttest_ind(your_model_scores, sklearn_model_scores)\n",
    "print(\"T-statistic:\", t_stat)\n",
    "print(\"P-value:\", p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "50.007_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
