{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid / Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The sigmoid function squishes all input values between 0 and 1. It is used to convert the output of a linear function\n",
    "into a probability, which is then used to make a binary decision. \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "z : numpy array\n",
    "\n",
    "Returns\n",
    "-------\n",
    "_ : numpy array\n",
    "    the predicted output\n",
    "'''\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The loss function is the overall loss function for logistic regression for this specific iteration. \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y : numpy array\n",
    "y_hat : numpy array\n",
    "\n",
    "Returns\n",
    "-------\n",
    "_ : float\n",
    "    the loss value of that particular iteration\n",
    "'''\n",
    "\n",
    "def log_likelihood(y, y_hat):\n",
    "    \n",
    "    return np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The gradients function calculates the partial derivatives of the loss function with respect to the weights and bias.\n",
    "This partial derivative is used to update the weights and bias in the direction that minimizes the loss function.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X, inputs : numpy array\n",
    "y, actual values : numpy array\n",
    "y_hat, hypothesis/predictions : numpy array\n",
    "\n",
    "Returns\n",
    "-------\n",
    "dw : float\n",
    "    partial derivative of the loss function with respect to the weights\n",
    "db : float\n",
    "    partial derivative of the loss function with respect to the bias\n",
    "'''\n",
    "\n",
    "def gradients(X, y, y_hat):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dw = (1 / m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1 / m) * np.sum((y_hat - y))\n",
    "\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The normalise function normalises the input features by subtracting the mean and dividing by the standard deviation.\n",
    "This helps to scale down the input features to a common scale, which helps in faster convergence of the gradient \n",
    "descent algorithm, and reduces the magnitude of the weights.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X, inputs : numpy array\n",
    "\n",
    "Returns\n",
    "-------\n",
    "X : numpy array\n",
    "    NumPy array of normalised input features\n",
    "'''\n",
    "\n",
    "def normalize(X):\n",
    "\n",
    "    epsilon=1e-8\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    X_normalized = (X - mean) / (std + epsilon)\n",
    "    \n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The train function trains the logistic regression model using the input features and target values. It uses the sigmoid \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X, inputs : numpy array\n",
    "y, actual values : numpy array\n",
    "bs, batch size : int\n",
    "epochs, number of iterations : int\n",
    "lr, learning rate : float\n",
    "\n",
    "Returns\n",
    "-------\n",
    "w : numpy.ndarray\n",
    "    The learned weights of the logistic regression model (shape: (n, 1)).\n",
    "b : float\n",
    "    The learned bias term of the logistic regression model.\n",
    "losses : list of floats\n",
    "    A list containing the loss values for each epoch during training.\n",
    "'''\n",
    "\n",
    "def train(X, y, bs, epochs, lr):\n",
    "    '''\n",
    "\n",
    "    m -> number of training examples\n",
    "    n -> number of features \n",
    "    w -> weights\n",
    "    b -> bias\n",
    "    losses -> list to store loss values\n",
    "    xb -> batch of input features for the specific batch\n",
    "    yb -> batch of target values for the specific batch\n",
    "\n",
    "    '''\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "    y = y.reshape(m, 1)\n",
    "    X = normalize(X)\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for i in range((m + bs - 1) // bs):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "\n",
    "        l = log_likelihood(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(-l)\n",
    "\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The predict function uses the learned weights and bias to make predictions on the input features.\n",
    "The inputs should be either the validation set or the test set.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X, inputs : numpy array\n",
    "w, learned weights : numpy array\n",
    "b, learned bias : float\n",
    "\n",
    "Returns\n",
    "-------\n",
    "_ : numpy array\n",
    "    the predicted output contating 0s and 1s.\n",
    "'''\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \n",
    "    X  = normalize(X)\n",
    "\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    \n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./data/train_tfidf_features.csv\")\n",
    "X_train = df_train.drop(['label', 'id'], axis=1)\n",
    "y_train = df_train['label']\n",
    "\n",
    "df_test = pd.read_csv(\"./data/test_tfidf_features.csv\")\n",
    "X_test = df_test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s:  1560\n",
      "Number of 0s:  2736\n",
      "[7097.788773589673, 6436.864793769081, 6135.905738817253, 5957.842324917137, 5838.262225236421, 5751.6554347193205, 5685.690592051076, 5633.6012277269265, 5591.335792802689, 5556.306448448964, 5526.775688721617, 5501.529592332729, 5479.692132198229, 5460.613950994216, 5443.80274765077, 5428.878045349482, 5415.540847088722, 5403.552718862369, 5392.721035450787, 5382.888364706548, 5373.924694763487, 5365.7216523030365, 5358.188139204538, 5351.246995368931, 5344.832414691791, 5338.8879212882375, 5333.364767777896, 5328.220655317268, 5323.418701643926, 5318.926602279795, 5314.715943620554, 5310.761636519208, 5307.041446244013, 5303.535600102161, 5300.226458089927, 5297.098235020761, 5294.136764952436, 5291.329300567021, 5288.664341586018, 5286.131487425136, 5283.721310180438, 5281.425244743685, 5279.23549341012, 5277.144942796976, 5275.14709125943, 5273.235985290319, 5271.40616363484, 5269.652608052571, 5267.970699824945, 5266.356181243597, 5264.805121429243, 5263.313885925964, 5261.879109595684, 5260.497672404608, 5259.166677750186, 5257.883433024861, 5256.645432153758, 5255.450339877933, 5254.295977584525, 5253.180310510405, 5252.101436167727, 5251.057573858496, 5250.047055161521, 5249.068315288964, 5248.11988522205, 5247.200384545807, 5246.308514912149, 5245.443054068461, 5244.602850395912, 5243.786817907937, 5242.99393166463, 5242.223223563633, 5241.473778472231, 5240.744730669078, 5240.0352605673, 5239.3445916934725, 5238.671987899755, 5238.016750788541, 5237.378217331094, 5236.7557576635145, 5236.148773044879, 5235.55669396388, 5234.978978381652, 5234.415110099487, 5233.864597241309, 5233.326970841634, 5232.801783530617, 5232.288608308494, 5231.787037402452, 5231.296681199536, 5230.817167249838, 5230.348139334535, 5229.889256594023, 5229.440192711614, 5229.000635148752, 5228.570284427977, 5228.148853460204, 5227.736066913173, 5227.331660618127, 5226.93538101208]\n"
     ]
    }
   ],
   "source": [
    "w, b, losses = train(X_train.values, y_train.values, bs=32, epochs=100, lr=0.01)\n",
    "\n",
    "y_hat = predict(X_test.values, w, b)\n",
    "num_ones = np.count_nonzero(y_hat)\n",
    "num_zeros = len(y_hat) - num_ones\n",
    "print(\"Number of 1s: \", num_ones)\n",
    "print(\"Number of 0s: \", num_zeros)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s:  1167\n",
      "Number of 0s:  3129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = Pipeline(steps=[('regressor', LogisticRegression())])\n",
    "df_train = pd.read_csv(\"./data/train_tfidf_features.csv\")\n",
    "X_train = df_train.drop(['label', 'id'], axis=1)\n",
    "y_train = df_train['label']\n",
    "\n",
    "df_test = pd.read_csv(\"./data/test_tfidf_features.csv\")\n",
    "X_test = df_test.drop(['id'], axis=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "num_ones = np.count_nonzero(y_hat)\n",
    "num_zeros = len(y_hat) - num_ones\n",
    "print(\"Number of 1s: \", num_ones)\n",
    "print(\"Number of 0s: \", num_zeros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "50.007_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
